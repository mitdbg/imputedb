\section{Algorithm}

At a high level, our goal is to select a query plan that minimizes a metric which combines the cost of the query and the quality of the results.
Precisely, we will find a query plan $Q$ that minimizes $\text{Cost}(Q) = \alpha \times \text{Time}(Q) + (1 - \alpha) \times \text{Loss}(Q)$. $\alpha$ is a parameter to the query optimizer that controls the emphasis on quality versus performance. $\alpha = 1.0$ means that the query should be as fast as possible, $\alpha=0.0$ means that the query should be as accurate as possible.

\subsection{Search space}
To reduce the size of the query plan search space, only plans that fit the following template are considered. First, all filters are pushed to the leaves of the query tree, immediately after the scans. Joins are performed after filtering, and only left-deep plans are considered. Any group-by/aggregate will be performed after the final join. Finally, projections are placed at the root of the query tree. The space of query plans is similar to that considered by Selinger \todo{Selinger cite}, with the addition of imputation operators.

\subsection{Imputation operators}
We introduce two new relational operators to perform imputation: impute ($\mu$) and drop ($\delta$). Each operator takes arguments $(C, R)$ where $C$ is a set of attributes and $R$ is a relation. Impute uses a machine learning method to replace all null values with non-null values for attributes in $C$ in the relation $R$. Drop simply removes all tuples which have a null value for some attribute in $C$ from $R$. Both operators guarantee that the resulting relation will contain no null values for attributes in $C$. 

\subsection{Imputation placement}
\label{sec:placement}
Imputation operators must be placed so that no relational operator receives a tuple containing null in an attribute that the operator examines, regardless of the state of the data in the base tables.

Imputation operators can be placed at any point in the query plan, but to meet the guarantee that no non-imputation operator sees a null value, there are cases where an imputation operator is required. To track these cases, each query plan is associated with a set of dirty attributes $D$. An attribute $c$ is \emph{dirty} in some relation if the values for $c$ contain null. We compute a dirty set for each base table using the table statistics, which track the number of null values in each column. If we apply an imputation operator to a relation $R$ with a dirty set $D$, $\mu_C (R)$ or $\delta_C (R)$, the resulting query has a dirty set $D' = D \setminus C$.  Applying a projection $\pi_C(R)$ produces a dirty set $D' = D \cap C$. A join $R_1 \Join_\psi R_2$ with dirty sets $D_1$ and $D_2$  produces a dirty set $D' = D_1 \cup D_2$.  Filters do not change the dirty set. 

The dirty set over-approximates the set of attributes that contain null. For example, a filter might remove all tuples which contain null without changing the dirty set, forcing an unnecessary imputation. We choose to over-approximate the dirty set to avoid the possibility of dropping a tuple that contains a null value without explicitly imputing the value or applying a drop operator.

\subsection{Query planning}
The input to our query planner is a tuple $(T, \Phi, \Psi, P, G, A)$: a set of tables $T$, a set of filter predicates $\phi_t, t \in T$, a set of join predicates $\psi_(t_1, t_2), t_1, t_2 \in T$, a set of attributes $P$, and an optional set of attributes $G$ and aggregator function $A \in \{\text{Max}, \text{Min}, \text{Sum}, \text{Avg}, \text{Count}\}$.

The query planner must select a join ordering in addition to placing imputation operators as described in Section~\ref{sec:placement}.

To reduce the search space, we only consider the minimal imputation, the maximal imputation, and the minimal drop. The minimal imputation (resp. drop) only imputes (resp. drops) the columns required by the relational operator immediately following the imputation. The maximal imputation imputes all columns in the relation, regardless of which are required. 

\begin{algorithm}
  \begin{algorithmic}
    \Require{$q$ is a query plan, $C_{req}$ is a set of attributes that must be imputed.}
    \Ensure{Returns a set of query plans such that $\Call{Dirty}{q'} \cap C_{req} = \emptyset$.}
    \Function{AddImpute}{$q, C_{req}$}
    \State $C_{min} \gets \Call{Dirty}{q} \cap C_{req}$
    \If{$C_{min} = \emptyset$}
    \State \Return $\{q\}$
    \Else
    \State \Return $\{\mu_{\Call{Dirty}{q}}(q), \mu_{C_{min}}(q), \delta_{C_{min}}(q)\}$
    \EndIf
    \EndFunction
    
    \State

    \Require{$Q$ is a set of query plans.}
    \Ensure{Returns an optimal query plan for each distinct dirty set in $Q$.}
    \Function{OptRel}{$Q$}
    \State $D \gets \{\Call{Dirty}{q} ~|~ q \in Q\}$
    \State \Return $\{\argmin_{q \in Q \land \Call{Dirty}{q} = d} \Call{Cost}{q} ~|~ d \in D\}$
    \EndFunction

    \State

    \Require{$t$ is a table and $\phi$ is a filter predicate.}
    \Ensure{Returns a set of optimal query plans for scanning and filtering $t$, with distinct dirty sets.}
    \Function{OptFilter}{$t, \phi$}
    \State \Return $\Call{OptRel}{\Call{AddImpute}{t, \Call{Attrs}{\phi}}}$
    \EndFunction

    \State
    
  
    \Require{$Q$ is a map from tables and dirty sets to optimal query plans (possibly with imputations), and $\Psi$ relates query plans with join predicates.}
    \Ensure{Returns a set of optimal query plans for all joins, with distinct dirty sets.}
    \Function{OptJoin}{$Q, \Phi$}
    \For{$size \in 1...|\Psi|$}
    	\State $S \gets subset(\Psi, size)$
	\For{$\phi \in S$}
		\State $S' \gets S \setminus \phi$
			\If{$joins(S', \phi)$}
				\State $S'_{plans} \gets Q(\Call{Rels}{S'})$
				\State $(t, \_) \gets \Call{Rels}{\phi}$
				\State $t_{plans} \gets Q(t)$
				\For {$l, r \in t_{plans} \times S'_{plans}$}
					\State $P_l \gets \Call{AddImpute}{l, \Call{Attrs}{\phi}}$
					\State $P_r \gets \Call{AddImpute}{r, \Call{Attrs}{\phi}}$
					\State \Call{Update}{Q, $\Call{OptRel}{ \{p_l \bowtie_{\phi} p_r | p_l \in P_l, p_r \in P_r \} }$}
				\EndFor
		\EndIf
	\EndFor
    \EndFor
    \State \Return{Q(\Call{Rels}{$\Phi$})}
    \EndFunction
  \end{algorithmic}
  \caption{An algorithm for query planning with imputations.}
\end{algorithm}

%\begin{verbatim}		
%# For a set of tables (with filter predicates) and join predicates, return the optimal plans.
%OptJoin(Tâ†, join_preds):
%	Q = empty
%	for t, pred in T:
%		S = T \ {t, pred}
%		# Get the optimal plans for the left and right arguments to the join.
%		for r_l, r_r in OptJoin(S) x OptFilter(t, pred):
%			if r_l, r_r in join_preds:
%				# Add imputation to the left and right arguments.
%				Q = Q union OptRel(Join(r_l', r_r') for r_l' in AddImpute(r_l, Attrs(join_pred)), r_r' in AddImpute(r_r, Attrs(join_pred)))
%	return OptRel(Q)
%# Extended cost calculation to account for information loss/computation cost in imputations
%Cost(q):
%case drop_min(q', d) =>  info_loss(drop, q', attrs(q') intersect d) + Cost(q')
%case impute_all(q') => info_loss(impute, q', attr(q')) + impute_cost(q') + Cost(q')
%case impute_min(q', d) => info_loss(impute, q', attrs(q') intersect d) + impute_cost(q'[d]) + Cost(q')
%case _ => normal cost calculation
%
%
%info_loss(op, t, a):
%# beta: some scaling factor
%case info_loss(drop, t, a) => beta * sum(for ct_missing(t.a) in a)
%case info_loss(impute, t, a) => beta *sum(for ct_missing(t.a) / ct_complete(t) in a)
%
%impute_cost(t):
%# alpha: some scaling factor
%alpha * (ntuples(t) + ct(attr(t)))
%\end{verbatim}

\subsection{Histogram computation}
In order to correctly estimate the imputation cost and computation time parameters in our cost model,  the system must have updated cardinality estimates
at each point in a query plan. These cardinality estimates can be impacted not just be filtering or joining, as in the traditional relational calculus, but also by
the imputation operators. For example,  dropping versus imputing tuples has a clearly different effect on cardinality. For simplicity of design,
each of the logical nodes in a query plan keeps a reference to a set of histograms. The optimizer creates a copy-on-write of the histograms and modifies them
as necessary to account for new operations in the plan. In all cases, modifications maintain as an invariant that the distribution of data in the new histogram
matches the distribution (but not necessarily the cardinality) of the original statistics. Algorithm~\ref{algo:histogram-transformation} provides the details
of our implementation.



\begin{algorithm}
  \begin{algorithmic}
    \Require{$H$ is a map from attribute names to histograms, $op$ is an operator node in a logical query plan}
    \Ensure{Returns an updated $H$, such that the distribution of data matches the original $H$}
    
    \Function{UpdateHistogram}{$H, op$}
    	\If{$op \not\in \{\delta, \mu, \sigma, \bowtie \}$}
		\Return{H}
	\EndIf
	\State $H' \gets \Call{Copy}{H}$
    	\If{op = $\delta_C$}
		\For{$c \in C$}
			\State $H'[c][null] \gets 0$
		\EndFor
	\ElsIf{op = $\mu_C$}
		\For{$c \in C$}
			\State \Call{Add}{$H'[c], H'[c][null]$}
		\EndFor
	\ElsIf{op = $\sigma$}
		\State \Call{ScaleBy}{H',  \Call{Selectivity}{$\sigma$}}
	\ElsIf{op = $\bowtie_\phi$}
		\State \Call{ScaleTo}{H', \Call{Cardinality}{$\bowtie_{\phi}$}}
	\EndIf
	\Return{H'}
    \EndFunction
  \end{algorithmic}
  \caption{An algorithm for in-plan histogram updates}
  \label{algo:histogram-transformation}
\end{algorithm}

\subsection{Imputation Strategies}
The current version of the system uses an imputation strategy based on chained-equations regression trees (CERT) \todo{CITE}. Note that plugging in a new
imputation strategy takes minimal effort, and doesn't require changes to the optimizer, so the system could in principle be tuned to a specific domain.
The imputation algorithm is iterative in nature, and proceeds by repeatedly fitting regression trees to a subset of the data and the target imputation columns.
In each iteration, the missing values of a column are replaced with newly imputed values. As the iterations proceed, the quality of the imputation
should improve as values progressively reflect more accurate relationships amongst attributes. Algorithm~\ref{algo:imputation-strategy}
provides details on the implementation.

It is worth noting that given the nature of the imputation strategy used here, imputation operators become blocking in our system.

\begin{algorithm}
  \begin{algorithmic}
    \Require{$T$ is a table. $D$ is a set of $T$ attributes that need to be imputed, and $C$ is a set of $T$ attributes that has complete data }
    \Ensure{Returns an imputed $T$}
    
    \Function{ImputeWithCERT}{$H, op$}
    	\State $T' \gets \Call{ImputeRandom}{T, D}$
	\For{$n\ EPOCHS$}
		\For{$d \in D$}
			\State $imp \gets \Call{TrainRT}{\pi_C(T'), \pi_d{T'}}$
			\State $T' \gets (\pi_C(T'), \Call{PredictRT}{imp, T})$
		\EndFor
	\EndFor
	\Return{T'}
	\EndFunction
  \end{algorithmic}
  \caption{An algorithm for in-plan histogram updates}
  \label{algo:histogram-transformation}
\end{algorithm}




\todo{Jose:Micah, take a look at the explanation above and see if it's missing anything (or is incorrect)}

\subsection{Complexity}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
